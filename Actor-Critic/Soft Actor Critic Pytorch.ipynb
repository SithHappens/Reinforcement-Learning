{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "   \n",
    "With this algorithm we want to solve the problem of how to get robust and stable learning in continuous action space environments.\n",
    "\n",
    "We can also use algorithms like DDPG or TD3 for continuous action environments, and TD3 in particular works really well, it's on par with the SAC algorithm (they were developed concurrently by separate groups), so they are comparable in terms of quality. DDPG falls short and doesn't do quite as well.\n",
    "\n",
    "The basic idea of SAC is to use a Maximum Entropy Framework (entropy just means disorder in this case), so it's gonna add a parameter to the cost function, which scales the cost function in such a way that it encourages exploration, but it does so in a way that is robust to random seeds for the environment, as well as episode to episode variatons and starting conditions. It is maximizing not just the total reward over time, but also the stochasticity, the randomness, the entropy of how the agent behaves.\n",
    "\n",
    "The Actor Network is the Policy. In DDPG or TD3, the network outputs the action directly, here we output the Mean and Standard Deviation for a Normal Distribution, which we then sample to get the actions for our agent.\n",
    "\n",
    "The Critic Network takes a State and Action as Input and \"judges\" the action taken by the actor.\n",
    "\n",
    "The Value Network assigns a value to the states.\n",
    "\n",
    "SAC lears rather slowly. The entropy comes from scaling the reward, is the scale factor grows, the signal to exploit grows, if the reward scale decreases, the tendency to explore increases. This is the only real parameter we have to play with for the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.size = max_size\n",
    "        \n",
    "        self.state_memory = np.zeros((self.size, *input_shape))\n",
    "        self.next_state_memory = np.zeros((self.size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.size)\n",
    "        self.terminal_memory = np.zeros(self.size, dtype=np.bool)\n",
    "        \n",
    "        self.counter = 0\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        idx = self.counter % self.size\n",
    "        \n",
    "        self.state_memory[idx] = state\n",
    "        self.next_state_memory[idx] = next_state\n",
    "        self.action_memory[idx] = action\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.terminal_memory[idx] = done\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, lr, input_dims, n_actions, fc1_dims=256, fc2_dims=256,\n",
    "                 name='critic', checkpoint_dir='tmp/sac'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dims[0] + n_actions, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, name + '_sac')\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        action_value = self.fc1(T.cat([state, action], dim=1))\n",
    "        action_value = F.relu(action_value)\n",
    "        action_value = self.fc2(action_value)\n",
    "        action_value = F.relu(action_value)\n",
    "        q = self.q(action_value)\n",
    "        return q\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, lr, input_dims, fc1_dims=256, fc2_dims=256,\n",
    "                 name='value', checkpoint_dir='tmp/sac'):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.v = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, name + '_sac')\n",
    "        \n",
    "    def forward(self, state):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        v = self.v(state_value)\n",
    "        return v\n",
    "        \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, lr, input_dims, n_actions, max_action, fc1_dims=256, fc2_dims=256,\n",
    "                 name='actor', checkpoint_dir='tmp/sac'):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.max_action = max_action\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        \n",
    "        self.reparam_noise = 1e-6  # so we do not take log(0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.sigma = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, name + '_s ac')\n",
    "        \n",
    "    def forward(self, state):\n",
    "        prob = self.fc1(state)\n",
    "        prob = F.relu(prob)\n",
    "        prob = self.fc2(prob)\n",
    "        prob = F.relu(prob)\n",
    "        \n",
    "        mu = self.mu(prob)\n",
    "        sigma = self.sigma(prob)\n",
    "        \n",
    "        # we could also use sigma activation to clamp between 0 and 1, but it is slower\n",
    "        sigma = T.clamp(sigma, min=self.reparam_noise, max=1)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        # reparameterize is a trick the authors use\n",
    "        mu, sigma = self.forward(state)\n",
    "        probabilities = Normal(mu, sigma)\n",
    "        \n",
    "        if reparameterize:\n",
    "            actions = probabilities.rsample()  # gives us sample + noise (additional exploration factor)\n",
    "        else:\n",
    "            actions = probabilities.sample()  # gives us sample\n",
    "            \n",
    "        action = T.tanh(actions) * T.tensor(self.max_action).to(self.device)  # scale the action beyond +- 1\n",
    "        log_probabilities = probabilities.log_prob(actions)\n",
    "        log_probabilities -= T.log(1 - action.pow(2) + self.reparam_noise)  # from the appendix of the paper\n",
    "        \n",
    "        # pytorch outputs a vector, but we need a scalar quantity for the loss\n",
    "        log_probabilities = log_probabilities.sum(1, keepdim=True)\n",
    "        \n",
    "        return action, log_probabilities\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, input_dims, n_actions, lr_alpha=0.0003, lr_beta=0.0003, gamma=0.99,\n",
    "                 max_size=1000000, tau=0.005, layer1_size=256, layer2_size=256, batch_size=256, reward_scale=2):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau  # factor by which we're going to modulate the parameters of the target value network\n",
    "                        # we have a value network and a target value network, and instead of a hard copy, we\n",
    "                        # do a soft copy, meaning we detune the parameters by this factor\n",
    "        \n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.actor = ActorNetwork(lr_alpha, input_dims, self.n_actions, max_action=env.action_space.high)\n",
    "        \n",
    "        # we take the min of the evaluation of the state for these two networks in the loss calculation\n",
    "        self.critic_1 = CriticNetwork(lr_beta, input_dims, self.n_actions, name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(lr_beta, input_dims, self.n_actions, name='critic_2')\n",
    "        \n",
    "        self.value = ValueNetwork(lr_beta, input_dims, name='value')\n",
    "        self.target_value = ValueNetwork(lr_beta, input_dims, name='target_value')\n",
    "        \n",
    "        self.scale = reward_scale\n",
    "        self.update_network_parameters(tau=1)  # hard copy in the beginning for target networks\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "        \n",
    "        return actions.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        target_value_params = dict(self.target_value.named_parameters())\n",
    "        value_params = dict(self.value.named_parameters())\n",
    "        \n",
    "        for name in value_params:\n",
    "            value_params[name] = tau * value_params[name].clone() + (1 - tau) * target_value_params[name].clone()\n",
    "\n",
    "        self.target_value.load_state_dict(value_params)\n",
    "        \n",
    "    def save_models(self):\n",
    "        print('Saving models')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.value.save_checkpoint()\n",
    "        self.target_value.save_checkpoint()\n",
    "        self.critic_1.save_checkpoint()\n",
    "        self.critic_2.save_checkpoint()\n",
    "        \n",
    "    def load_models(self):\n",
    "        print('Loading models')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.value.load_checkpoint()\n",
    "        self.target_value.load_checkpoint()\n",
    "        self.critic_1.load_checkpoint()\n",
    "        self.critic_2.load_checkpoint()\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.counter  < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        state = T.tensor(state, dtype=T.float).to(self.actor.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.actor.device)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "        next_state = T.tensor(next_state, dtype=T.float).to(self.actor.device)\n",
    "        done = T.tensor(done).to(self.actor.device)\n",
    "        \n",
    "        value = self.value(state).view(-1)\n",
    "        target_value = self.target_value(next_state).view(-1)\n",
    "        target_value[done] = 0.0\n",
    "        \n",
    "        actions, log_probs = self.actor.sample_normal(state, reparameterize=False)\n",
    "        log_probs = log_probs.view(-1)\n",
    "        \n",
    "        # improves stability of learning (overestimation bias due to the max in the Q Update), see TD3 paper\n",
    "        q1_new_policy = self.critic_1(state, actions)\n",
    "        q2_new_policy = self.critic_2(state, actions)\n",
    "        critic_value = T.min(q1_new_policy, q2_new_policy).view(-1)\n",
    "        \n",
    "        self.value.optimizer.zero_grad()\n",
    "        value_target = critic_value - log_probs\n",
    "        value_loss = 0.5 * F.mse_loss(value, value_target)\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        self.value.optimizer.step()\n",
    "        \n",
    "        actions, log_probs = self.actor.sample_normal(state, reparameterize=True)\n",
    "        log_probs = log_probs.view(-1)\n",
    "        \n",
    "        q1_new_policy = self.critic_1(state, actions)\n",
    "        q2_new_policy = self.critic_2(state, actions)\n",
    "        critic_value = T.min(q1_new_policy, q2_new_policy).view(-1)\n",
    "        \n",
    "        actor_loss = log_probs - critic_value\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor.optimizer.step()\n",
    "        \n",
    "        self.critic_1.optimizer.zero_grad()\n",
    "        self.critic_2.optimizer.zero_grad()\n",
    "        q_hat = self.scale * reward + self.gamma * target_value\n",
    "        q1_old_policy = self.critic_1(state, action).view(-1)\n",
    "        q2_old_policy = self.critic_2(state, action).view(-1)\n",
    "        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n",
    "        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n",
    "        \n",
    "        critic_loss = critic_1_loss + critic_2_loss\n",
    "        critic_loss.backward()\n",
    "        self.critic_1.optimizer.step()\n",
    "        self.critic_2.optimizer.step()\n",
    "        \n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/Dokumente/python_venv/ai/lib/python3.5/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models\n",
      "Episode   0, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   1, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   2, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   3, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   4, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   5, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   6, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   7, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   8, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode   9, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode  10, Score: 1000.0, Avg Score: 1000.0\n",
      "Episode  11, Score: 1000.0, Avg Score: 1000.0\n"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('InvertedPendulumBulletEnv-v0')\n",
    "agent = Agent(env, input_dims=env.observation_space.shape, n_actions=env.action_space.shape[0])\n",
    "\n",
    "n_games = 250\n",
    "filename = 'inverted_pendulum.png'\n",
    "figure_file = 'plots/' + filename\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_checkpoint = True\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    env.render(mode='human')\n",
    "    \n",
    "for i in range(n_games):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "            \n",
    "    print('Episode {:3}, Score: {:5.1f}, Avg Score: {:5.1f}'.format(i, score, avg_score))\n",
    "    \n",
    "if not load_checkpoint:\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
